---
title: "â€œLab 4: Data Wrangling: Cleaning, Preparation, and Tidying Data"
author: "Nabila Nishat Raisa"
format: pdf
editor: visual
execute:
  echo: true
---

## Installing packages and Prep

test:

```{r}
1 + 1
```

```{r}
#install.packages("nycflights13")
#library(nycflights13)
```

# Part 1: Primary Data Wrangling Verbs in dplyr and tidyr

```{r}
library(tibble)
as_tibble(iris)

```

```{r}
glimpse(iris)

```

### Deliverable 1

```{r}
library(dplyr)
iris %>%
group_by(Species) %>%
summarise(avg = mean(Sepal.Width)) %>%
arrange(avg)

```

```{r}
filter(iris, Sepal.Length >7)
```

```{r}
distinct(iris)

```

### Deliverable 2

```{r}
sample_frac(iris, 0.5, replace = TRUE)

```

```{r}
sample_n(iris, 10, replace = TRUE)

```

```{r}
slice(iris, 10:15)

```

```{r}
top_n(storms, 2, day)

```

### Deliverable 3

```{r}
summarize(iris, avg = mean(Sepal.Length))
mutate_each(iris, funs = mean)
count(iris, Species, wt = Sepal.Length)

```

```{r}
nycflights13::flights

```

```{r}
library(nycflights13)
filter(flights, month == 1, day ==1)

```

```{r}
jan1 <- filter(flights, month == 1, day ==1)

```

```{r}
(dec25 <- filter(flights, month == 12, day == 25))
nrow(dec25)

#there were 719 flights on December 25th, 2013. It's also displayed in the tibble.

```

```{r}
filter(flights, month == 1)

```

```{r}
filter(flights, month == 11 | month == 12)
#around 55.5 thousand flights departed in either November or December. 
filter(flights, month == 11)
#around 27 thousand flights departed in November. 
filter(flights, month == 12)
#around 28 thousand flights departed in November. 



```

```{r}
nov_dec <- filter(flights, month %in% c(11,12))

```

```{r}
arrange(flights, year, month, day)

```

```{r}
arrange(flights, desc(arr_delay))

```

```{r}
select(flights, year, month, day)

```

```{r}
select(flights, year:day)

```

```{r}
select(flights, -(year:day))

```

```{r}
rename(flights, tail_num = tailnum)

```

### Deliverable 4

```{r}
flights_sml <- select(flights, year:day, ends_with("delay"),distance, air_time)

```

```{r}
mutate(flights_sml, gain = arr_delay - dep_delay, speed = distance/air_time*60)

```

```{r}
mutate(flights_sml, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

```{r}
transmute(flights, gain = arr_delay - dep_delay, hours = air_time/60, gain_per_hour = gain/hours)

```

```{r}
summarize(flights, delay = mean(dep_delay, na.rm=TRUE))

```

```{r}
by_day <- group_by(flights, year, month, day)
summarize(by_day, delay = mean(dep_delay, na.rm=TRUE))
```

```{r}
library(ggplot2)
by_dest <- group_by(flights, dest)
delay <- summarize(by_dest, count=n(), dist=mean(distance, na.rm=TRUE),
delay=mean(arr_delay, na.rm=TRUE))
delay <- filter(delay, count >20, dest != "HNL")
ggplot(data = delay, mapping = aes(x=dist, y=delay))+
geom_point(aes(size=count), alpha = 1/3)+
geom_smooth(se=FALSE)

```

### Deliverable 5

```{r}
delays <- flights %>%
group_by(dest) %>%
summarize(
count=n(),
dist=mean(distance, na.rm=TRUE),
delay=mean(arr_delay, na.rm=TRUE)) %>%
filter(count > 20, dest != "HNL")

```

```{r}
daily <- group_by(flights, year, month, day)
(per_day <- summarize(daily, flights=n()))
```

```{r}
daily %>%
ungroup() %>%
summarize(flights=n())
```

# Part 2: Handling Missing Values with dplyr

### Deliverable 6

```{r}
flights %>%
group_by(year, month, day) %>%
summarize(mean=mean(dep_delay))
```

```{r}
flights %>%
group_by(year, month, day) %>%
summarize(mean=mean(dep_delay, na.rm=TRUE))

```

# Part 3: Practicing Data Wrangling on Real Text Mining Projects

```{r}
library(readr)
impeachtidy <- read_tsv("impeach.tab")

```

### Deliverable 7

```{r}
library(tidytext)
impeach_words <- impeachtidy %>%
  unnest_tokens(word,TEXT)

```

```{r}
data(stop_words)
head(stop_words)
tail(stop_words)

```

### Deliverable 8

```{r}
impeach_clean <- impeach_words %>%
anti_join(stop_words)
```

```{r}
impeach_clean
```

### Deliverable 9

```{r}
impeach_clean %>%
count(word, sort = TRUE)

#the top ten words in order from this clean dataset are president,ukraine,ambassador,trump,call, zelensky, correct,meeting,time, sondland

```

### Deliverable 10

```{r}
impeach_clean %>%
count(word, sort = TRUE) %>%
filter(n>600) %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n)) +
geom_col() +
xlab(NULL) +
coord_flip()

```

### Deliverable 11

```{r}
impeachtidy <- read_tsv("impeach.tab")
impeach_words <- impeachtidy %>%
unnest_tokens(word,TEXT) %>%
anti_join(stop_words)
impeach_clean <- impeach_words %>%
anti_join(stop_words)
impeach_clean %>%
count(word, sort = TRUE) %>%
filter(n>600) %>%
mutate(word=reorder(word,n)) %>%
ggplot(aes(word,n)) +
geom_col() +
xlab(NULL) +
coord_flip()
```

```{r}
impeach_words <- impeachtidy %>%
unnest_tokens(word,TEXT) %>%
count(SPEAKER, word, sort=TRUE) %>%
ungroup()

```

### Deliverable 12

```{r}
total_impeach <- impeach_words %>%
group_by(SPEAKER) %>%
summarize(total=sum(n)) %>%
arrange(desc(total))
total_impeach
```

```{r}
total_impeach %>%
ggplot(aes(SPEAKER,total)) +
geom_col() +
xlab(NULL) +
ylab(NULL) +
coord_flip()

```

```{r}
total_impeach %>%
ggplot(aes(SPEAKER,total)) +
geom_col() +
xlab(NULL) +
ylab(NULL)
```

### Deliverable 13

```{r}
library(tm)
igfbali <- Corpus(DirSource("C:/Users/auuser/OneDrive - american.edu/ITEC 724/Lab 4/txt_data/txt_data"), readerControl=list(reader=readPlain))


```

```{r}
#class(igfbali)


```

### Deliverable 14

```{r}
library(tm)
igfbali <- tm_map(igfbali, removeWords, stopwords("en"))

```

```{r}
more.stop.words <- c("transcript", "transcripts")

```

```{r}
combined_stopwords <- c(stopwords("en"), more.stop.words)

# Remove combined stopwords
igfbali <- tm_map(igfbali, removeWords, combined_stopwords)

#took help from chatgpt as code was not working
```

```{r}
tm_map(igfbali, stemDocument)

```

### Deliverable 15

```{r}
dtm <- DocumentTermMatrix(igfbali)

```

### Deliverable 16

```{r}
findFreqTerms(dtm, 500)

```

```{r}
#inspect(removeSparseTerms(dtm, sparse=0.4))
#commented out as output is too long

```

### Deliverable 17

```{r}
findAssocs(dtm, "activists", 0.8)
findAssocs(dtm, "cybersecurity", 0.8)

```

```{r}
inspect(DocumentTermMatrix(igfbali, list(dictionary = c("multistakeholder", "freedom", "development"))))

```

# Part 4: Introduction to Data Wrangling in Python

### Deliverable 18

```{python}
from nltk.corpus import reuters
print("Categories:", reuters.categories())
print("Number of documents:", len(reuters.fileids()))

```

```{python}
doc_id = reuters.fileids(categories="crude")[0]
doc_text = reuters.raw(doc_id)

```

```{python}
import string
cleaned_text = doc_text.translate(str.maketrans('', '', string.punctuation))
cleaned_text = ' '.join(cleaned_text.split())
print(cleaned_text)
```

### Deliverable 19

```{python}
nltk.download('punkt_tab')
```

```{python}

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
tokens = word_tokenize(cleaned_text)
tokens = [word for word in tokens if word not in stopwords.words('english')]
print(tokens)



```

```{python}
from nltk.stem import PorterStemmer, WordNetLemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
stemmed = [stemmer.stem(word) for word in tokens]

nltk.download('wordnet')

lemmatized = [lemmatizer.lemmatize(word) for word in tokens]
print("Stemmed:", stemmed)
print("Lemmatized:", lemmatized)
```

### Deliverable 20

```{python}
from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')
tagged_tokens = pos_tag(tokens)
print(tagged_tokens)

```

### Deliverable 21

```{python}
def preprocess_pipeline(text):
  text = text.lower().translate(str.maketrans('', '', string.punctuation))
  text = ' '.join(text.split())
  tokens = word_tokenize(text)
  tokens = [word for word in tokens if word not in stopwords.words('english')]
  lemmatized = [lemmatizer.lemmatize(word) for word in tokens]
  tagged = pos_tag(lemmatized)
  return tagged

doc_text = reuters.raw(reuters.fileids(categories='crude')[0])
processed = preprocess_pipeline(doc_text)
print(processed)
```
