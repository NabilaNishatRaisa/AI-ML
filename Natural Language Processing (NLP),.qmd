---
title: "Lab_8 Natural Language Processing (NLP), Parts of Speech Tagging (POS), and Named Entity Recognition (NER)"
author: "Nabila Nishat"
format: pdf
editor: visual
execute: 
  echo: true 
---

## Pre-lab

```{r}
# options(timeout = 300)  # Increase timeout to 300 seconds
# install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")

```

```{r}
#install.packages(c("openNLP", "sentimentr", "coreNLP", "cleanNLP", "magrittr", "NLP", "gridExtra", "ggthemes", "purrr", "doBy", "cshapes", "rJava", "sotu", "spacyr", "tinytex", "sf"))


```

```{r}
#Sys.setenv(JAVA_HOME = "C:/Program Files/Java/jdk-xx") 

```

```{r}

# library(openNLP)
# library(sentimentr)
# 
# library(coreNLP) 
# library(cleanNLP)
# library(magrittr)
# library(NLP)
# library(gridExtra)
# library(ggthemes) 
# library (purrr)
# library(doBy)
# library(cshapes)
# library(sotu)
# library(spacyr)
# library(tinytex)
# library(sf)


```

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale("LC_ALL","C")
lib.loc = "~/R/win-library/3.2"

```

```{r}
# install.packages("spacyr")
# library(spacyr)

```

```{r}
# spacy_install(
# conda = "auto",
# version = "latest",
# lang_models = "en_core_web_sm",
# python_version = "3.11.5",
# envname = "textmining",
# pip = FALSE,
# python_path = "/Users/auuser/anaconda3/envs/Lab3/python.exe",
# prompt = TRUE
# )
```

```{r}
getwd()
```

## Part 1: Parts of Speech (POS) Tagging in openNLP

#### Deliverable 2: Create Some Sample Text

```{r}
library(NLP)
s <- paste(c('Pierre Vinken, 61 years old, will join the board as a ',
'nonexecutive director Nov 29.',
'Mr. Vinken is chairman of Elsevier, N.V., ',
'the Dutch publishing group.'),
collapse = '')
s <- as.String(s)
s

```

#### Deliverable 3: Create Sentence and Word Token Annotations

```{r}
library(openNLP)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()

```

```{r}
a2<- annotate(s, list(sent_token_annotator, word_token_annotator))
a2

```

```{r}
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(s, pos_tag_annotator, a2)
a3
a3w <- subset(a3, type == "word")
a3w
```

```{r}
tags<-sapply(a3w$features,"[[","POS")
tags

```

```{r}
tags <- sapply(a3w$features,"[[","POS")
tags
table(tags)
```

#### Deliverable 3: Extract Tokens/POS Pairs

```{r}
sprintf("%s/%s", s[a3w], tags)

```

## Part 2: Applying Named Entity Recognition (NER) to the

#### Deliverable 4: Import Data and Organize the Clinton Email Dataset

```{r}
# install.packages("pbapply")
library(pbapply)
# 
tmp <- list.files(path ="/Users/auuser/OneDrive - american.edu/Fall 2024/ITEC 724/Lab 8/C8_final_txts/", pattern = '*.txt', full.names = T)
emails <- pblapply(tmp, readLines)
names(emails) <- gsub('.txt', '', list.files(pattern = '.txt'))
```

#### Deliverable 5: Examine the Clinton Email Dataset

```{r}
emails[[1]]

```

#### Deliverable 6: Create a Custom Function to Clean the Emails

```{r}
txtClean <- function(x) {
x <- x[-1]
x <- paste(x,collapse = " ")
x <- str_replace_all(x, "[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+", "")
x <- str_replace_all(x, "Doc No.", "")
x <- str_replace_all(x, "UNCLASSIFIED U.S. Department of State Case No.", "")
x <- removeNumbers(x)
x <- as.String(x)
return(x)
}
```

#### Deliverable 7: Apply the Cleaning Function to the Clinton Email Dataset

```{r}
library(stringr)
library(tm)
txtClean(emails[[1]])[[1]]

```

```{r}
allEmails <- pblapply(emails,txtClean)
allEmails[[2]][[1]][1]
```

#### Deliverable 8: Apply POS Tagging to the Clinton Email Dataset

```{r}
library(NLP)
library(openNLP)
library(openNLPmodels.en)  

```

```{r}
persons <- Maxent_Entity_Annotator(kind='person')
locations <- Maxent_Entity_Annotator(kind='location')
organizations <- Maxent_Entity_Annotator(kind='organization')
```

```{r}
sentTokenAnnotator <- Maxent_Sent_Token_Annotator(language='en')
wordTokenAnnotator <- Maxent_Word_Token_Annotator(language='en')
```

```{r}
posTagAnnotator <- Maxent_POS_Tag_Annotator(language='en')

```

#### Deliverable 9: Annotate the Clinton Email Dataset Using a For Loop

```{r}
annotationsData <- list()
for (i in 1:length(allEmails)){
  print(paste('starting annotations on doc', i))
  annotations <- annotate(allEmails[[i]], list(sentTokenAnnotator,
                                                wordTokenAnnotator,
                                                posTagAnnotator,
                                                persons,
                                                locations,
                                                organizations))
annDF <- as.data.frame(annotations)[,2:5]
annDF$features <- unlist(as.character(annDF$features))
annotationsData[[tmp[i]]] <- annDF
print(paste('finished annotations on doc', i))
}
```

#### Deliverable 10: Extract Terms from the Clinton Email Dataset

```{r}
allData<- list()
for (i in 1:length(allEmails)){
x <- allEmails[[i]] # get an individual document
y <- annotationsData[[i]] # get an individual doc's annotation information
print(paste('starting document:',i, 'of', length(allEmails)))
# for each row in the annotation information, extract the term by index
POSls <- list()
for(j in 1:nrow(y)){
annoChars <- ((substr(x,y[j,2],y[j,3]))) #substring position
# Organize information in data frame
z <- data.frame(doc_id = i,
type = y[j,1],
start = y[j,2],
end = y[j,3],
features = y[j,4],
text = as.character(annoChars))
POSls[[j]] <- z
#print(paste('getting POS:', j))
}
# Bind each documents annotations & terms from loop into a single DF
docPOS <- do.call(rbind, POSls)
# So each document will have an individual DF of terms, and annotations as a list element
allData[[i]] <- docPOS
}
```

#### Deliverable 11: Subset the Clinton Email Dataset

```{r}
people <- pblapply(allData, subset, grepl("*person", features))
locaction <- pblapply(allData, subset, grepl("*location", features))
organization <- pblapply(allData, subset, grepl("*organization", features))
people
locaction 
organization
POSdf <- do.call(rbind, allData)
```

#### Deliverable 12: Using the Annotate Entities Process

```{r}
annDF
subset(annDF$words, grepl("*people", annDF$features) == T)
subset(annDF$words, grepl("*locaction", annDF$features) == T)
subset(annDF$words, grepl("*organization", annDF$features) == T)
person
```

#### Deliverable 13: Annotate Entities with OpenNLP

```{r}
annotate.entities <- function(doc, annotation.pipeline) {
annotations <- annotate(doc, annotation.pipeline)
AnnotatedPlainTextDocument(doc, annotations)
}
ner.pipeline <- list(
Maxent_Sent_Token_Annotator(),
Maxent_Word_Token_Annotator(),
Maxent_POS_Tag_Annotator(),
Maxent_Entity_Annotator(kind = "person"),
Maxent_Entity_Annotator(kind = "location"),
Maxent_Entity_Annotator(kind = "organization")
)

```

#### Deliverable 14: Apply the Annotation Function

```{r}
library(pbapply)
all.ner <- pblapply(allEmails, annotate.entities, ner.pipeline)
all.ner <- pblapply(allEmails, annotate.entities, ner.pipeline)
```

```{r}
library(purrr)
all.ner
all.ner <- pluck(all.ner, "annotations")
all.ner <- pblapply(all.ner, as.data.frame)




```

## Part 3: Conducting NLP Analysis with spacyr

#### Deliverable 15: Load the spacyr Package

```{r}
library(spacyr)
txt <- c(d1 = "spaCy is great at fast natural language processing.",
d2 = "Mr. Smith spent two years in North Carolina.")
parsedtxt <- spacy_parse(txt)
parsedtxt

```

#### Deliverable 16: Review the Parsed Text

```{r}
spacy_parse(txt, tag = TRUE, entity = FALSE, lemma = FALSE)


```

#### Deliverable 17: Parse the txt Object with spacyr

```{r}
spacy_tokenize(txt)

```

#### Deliverable 18: Tokenize Text into a Data Frame

```{r}
library(dplyr)
spacy_tokenize(txt, remove_punct = TRUE, output = "data.frame") %>%
tail()

```

#### Deliverable 19: Extract Named Entities From Parsed Text

```{r}
parsedtxt <- spacy_parse(txt, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
entity_extract(parsedtxt)

```

#### Deliverable 20: Extract Extended Entity Set

```{r}
entity_extract(parsedtxt, type = "all")

```

#### Deliverable 21: Consolidated named entities

```{r}
entity_consolidate(parsedtxt) %>%
tail()

```

#### Deliverable 22: Extract Noun Phrases

```{r}
nounphrase_extract(parsedtxt)

```

```{r}
nounphrase_consolidate(parsedtxt)

```

#### 

#### Deliverable 23: Extract Entities Without Parsing the Entire Text

```{r}
spacy_extract_entity(txt)

```

```{r}
spacy_extract_nounphrases(txt)

```

```{r}
spacy_parse(txt, dependency = TRUE, lemma = FALSE, pos = FALSE)

```

```{r}
spacy_parse("I have six email addresses, including me@mymail.com.",
additional_attributes = c("like_num", "like_email"),
lemma = FALSE, pos = FALSE, entity = FALSE)
```

#### Deliverable 24: Extract Additional Attributes

```{r}
library(quanteda, warn.conflicts = FALSE, quietly = TRUE)
# To identify the names of the documents
docnames(parsedtxt)
# To count the number of tokens in the documents
ntoken(parsedtxt)
# To count the number or types of tokens
ntype(parsedtxt)

```

#### Deliverable 25: Extract Additional Attributes

```{r}
parsedtxt <- spacy_parse(txt, pos = TRUE, tag = TRUE)
as.tokens(parsedtxt)

```

```{r}
spacy_parse("The cat in the hat ate green eggs and ham.", pos = TRUE) %>%
as.tokens(include_pos = "pos") %>%
tokens_select(pattern = c("*/NOUN"))

```

```{r}
spacy_tokenize(txt) %>%
as.tokens()
```

```{r}
txt2 <- "A Ph.D. in Washington D.C. Mr. Smith went to Washington."
spacy_tokenize(txt2, what = "sentence") %>%
as.tokens()
```

```{r}
spacy_parse(txt, entity = TRUE) %>%
entity_consolidate() %>%
as.tokens() %>%
head(1)

```

#### Deliverable 26: Using spacyr with tidytext

```{r}
library("tidytext")
unnest_tokens(parsedtxt, word, token) %>%
dplyr::anti_join(stop_words)

```

#### Deliverable 27: POS Filtering

```{r}
spacy_parse("The cat in the hat ate green eggs and ham.", pos = TRUE) %>%
unnest_tokens(word, token) %>%
dplyr::filter(pos == "NOUN")

```

#### Deliverable 28: Finalizing the SpaCy Connection

```{r}
 spacy_finalize()
```

```

#### 
