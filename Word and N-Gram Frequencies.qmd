---
title: "Lab 2"
author: "Nabila Nishat Raisa"
format: pdf
editor: visual
execute:
  echo: true
---

## Testing R and Installing Packages

```{r}
1+1
```

# Define the Problem

Questions:

1\. What is the average length of a social customer service reply?

2\. What links were referenced most often?

3\. How many people should be on a social media customer service team?

4\. How many social replies are reasonable for a customer service representative to handle?

### Dataset details:

The name of the dataset is “oct_delta.csv”. It is a collection of Delta tweets from the Twitter API from 1 October 1 to 15 October 2015. It has been cleaned up for easier analysis.

# Part 1: Importing and Exploring Text Data in BaseR

```{r}
options(stringsAsFactors = FALSE)
Sys.setlocale("LC_ALL","C")
```

```{r}
data()
```

```{r}
#1
read.csv('oct_delta.csv')

```

```{r}
#2
text.df <- read.csv('oct_delta.csv')
```

```{r}
#4
#text.df 
  #weekday = c("Thu","Fri","Sat"),
  #month = c("Oct"),
  #date = as.Date(c(1,2,3)),
  #year = c('2015'),
  #text = c("Sample text 1", "Sample text 2")

```

```{r}
#5
head(text.df)
tail(text.df)
class(text.df)
summary(text.df)
```

```{r}
#6
nchar(head(text.df$text))

```

```{r}
head(text.df$text, 6)
```

```{r}
#7
#The numbers are showing how many characters each sentences have. For examples, in the first sentence, "@mjdout I know that can be frustrating..we hope to have you parked and deplaned shortly. Thanks for your patience", there are 119 characters. The others numbers are showing the character totals for the other sentences respectively. 
```

```{r}
#8
index_example <- 1:50
index_example
```

```{r}
#9
nchar(text.df[4,5])
```

# Part 2: Extract Features from Data with the tm Package

```{r}
#10
mean(nchar(text.df$text))
# the average length of a Delta social customer service reply in October is 92.16412
#this is the average number of characters in all tweets. By length, we are referring to how long the reply is and not the time it took get reply. 

```

```{r}
#11
tweets <- data.frame(ID=seq(1:nrow(text.df)),text=text.df$text)
tweets

```

```{r}
#12
library(tm)
corpus <- VCorpus(VectorSource(tweets),readerControl =
readDataframe(tweets,"en",id = ID))

```

```{r}
inspect(corpus[1:2])
inspect(corpus[[2]])

```

```{r}
#13
dtm <- DocumentTermMatrix(corpus,control=list(weighting=weightTf))

dtm.tweets.m <- as.matrix(dtm)
term.freq <- rowSums(dtm.tweets.m)
freq.df <- data.frame(word=names(term.freq),frequency=term.freq)
freq.df <- freq.df[order(freq.df[,2],decreasing = T),]

```

```{r}
#14 took help from Chatgpt for changing string to factor
library(ggplot2)
freq.df <- data.frame(word = names(term.freq), frequency = term.freq)

freq.df <- freq.df[order(freq.df$frequency, decreasing = TRUE), ]

freq.df$word <- factor(freq.df$word, levels = freq.df$word)

plot <- ggplot(freq.df, aes(x = word, y = frequency)) + 
  geom_bar(stat = "identity") + 
  labs(title = "Word Frequency in Delta Tweets", 
       x = "Words", 
       y = "Frequency") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(plot)
```

```{r}
#15
library(ggthemes)
freq.df$word <- factor(freq.df$word,levels = unique(as.character(freq.df$word)))
ggplot(freq.df[1:20,], aes(x=word, y=frequency)) +
geom_bar(stat="identity", fill="darkred") +
coord_flip() +
theme_gdocs() +
geom_text(aes(label=frequency), colour="white",hjust=1.25, size=5.0)

```

# Part 3: Introduction to the Tidyverse and Tidytext

```{r}
#16
library(dplyr)
library(ggplot2)
library(tidytext)
library(janeaustenr)
library(stringr)

data("austen_books", package = "tidytext")
head(austen_books())

original_books <- austen_books() %>%
group_by(book) %>% mutate(linenumber=row_number(),
chapter=cumsum(str_detect(text,regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  
ungroup()

```

```{r}
original_books
class(original_books)
```

```{r}
#17
tidy_books <- original_books %>%
unnest_tokens(word, text)
```

```{r}
tidy_books
class(tidy_books)
```

```{r}
#17
data(stop_words)
stop_words
```

```{r}
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books

```

```{r}
#19
tidy_books %>%
count(word, sort = TRUE)

```

```{r}
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word=reorder(word, n)) %>%
ggplot(aes(word,n)) +
geom_col() +
xlab(NULL) +
coord_flip()
```

```{r}
#20
library(gutenbergr)
hgwells <- gutenberg_download(c(35,36,5230,159))
hgwells

```

```{r}
#21
tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text)

print(tidy_hgwells)
```

```{r}
#22
tidy_hgwells <- hgwells %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_hgwells
```

```{r}
#23
tidy_hgwells %>%
count(word, sort = TRUE)

```

```{r}
#24
bronte <- gutenberg_download(c(1260,768,969,9182,767))
```

```{r}
#25
tidy_bronte <- bronte %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
```

```{r}
#26
tidy_bronte %>%
count(word, sort = TRUE)
```

```{r}
#27
library(tidyr)
frequency <- bind_rows(mutate(tidy_bronte, author="Bronte Sisters"),
mutate(tidy_hgwells, author="H.G. Wells"),
mutate(tidy_books, author="Jane Austen")) %>%
mutate(word = str_extract(word, "[a-z'] +")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, "Bronte Sisters":"H.G. Wells")

```

```{r}
#28
library(scales)
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color =
abs(`Jane Austen` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4",
high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Jane Austen", x = NULL)

```

# Part 4: Word and N-Gram Frequencies using Python

```{python}
import pandas as pd
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
import matplotlib.pyplot as plt

```

```{python}
tweets_df = pd.read_csv('oct_delta.csv')
```

```{python}
tweets_df['char_count'] = tweets_df['text'].apply(len)

```

```{python}
tweets_df['char_count'].plot.hist(bins=20)
plt.show()

```

```{python}
cv = CountVectorizer()
tf = cv.fit_transform(tweets_df['text'])
tf_feature_names = cv.get_feature_names_out()
```

```{python}
tf_df = pd.DataFrame(tf.toarray(), columns=tf_feature_names)

```

```{python}
tf_df.sum().sort_values(ascending=False).head(20).plot.bar()
plt.show()

```

```{python}
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(tweets_df['text'])
tfidf_feature_names = tfidf.get_feature_names_out()
```

```{python}
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_feature_names)
```

```{python}
tfidf_df.sum().sort_values(ascending=False).head(20).plot.bar()
plt.show()

```

```{python}
tf_df.sum().sort_values(ascending=False).head(20)

```

```{python}
tf_df.sum().sort_values(ascending=False).head(20).plot.bar()
plt.show()

```

```{python}
bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))
bigram_matrix = bigram_vectorizer.fit_transform(tweets_df['text'])
```

```{python}
trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))
trigram_matrix = trigram_vectorizer.fit_transform(tweets_df['text'])

```

```{python}
def plot_most_common_words(count_data, count_vectorizer, top_n=20):
  words = count_vectorizer.get_feature_names_out()
  total_counts = count_data.sum(axis=0).tolist()[0]
  count_dict = (zip(words, total_counts))
  count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:top_n]
  words, counts = zip(*count_dict)
  plt.figure(figsize=(10, 5))
  plt.bar(words, counts)
  plt.xticks(rotation=45)
  plt.show()
  
  
plot_most_common_words(tf, cv)
```

```{python}
nltk.download('gutenberg')
from nltk.corpus import gutenberg


austen_texts = gutenberg.raw(fileids=[f for f in gutenberg.fileids() if 'austen' in f])

```

```{python}
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')  

stop_words = set(stopwords.words('english'))

austen_texts = "Your sample text goes here."

austen_texts = word_tokenize(austen_texts)

filtered_words = [word for word in austen_texts if word.lower() not in stop_words]

```

```{python}
from sklearn.feature_extraction.text import CountVectorizer

texts = ["This is a sample text.", "Another text example."]

vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

feature_names = vectorizer.get_feature_names_out()

```

```{python}
tf_df.sum().sort_values(ascending=False).head(20)
plot_most_common_words(tf, cv)
plot_most_common_words(tfidf_matrix, tfidf)

```
